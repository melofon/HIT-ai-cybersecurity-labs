{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "157b104b-b1b2-44e1-8052-b6b32505dde6",
   "metadata": {},
   "source": [
    "# Event Producer (Windows Logs → Kafka)\r\n",
    "\r\n",
    "## Purpose of This Notebook\r\n",
    "\r\n",
    "This notebook implements the **event producer** for the cybersecurity pipeline used in this laboratory.\r\n",
    "\r\n",
    "Its purpose is to **simulate a real-world source of security-relevant events** and to inject these events into the pipeline **asynchronously**, using a message queue (Kafka).\r\n",
    "\r\n",
    "This notebook does **not** perform any detection or analysis.  \r\n",
    "It represents the *upstream data source* of the system.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## What This Producer Represents\r\n",
    "\r\n",
    "Conceptually, this notebook simulates:\r\n",
    "\r\n",
    "- Windows process execution logs  \r\n",
    "- Windows authentication events  \r\n",
    "- Host- and user-related security telemetry  \r\n",
    "\r\n",
    "In a real SOC / SIEM system, similar data would come from:\r\n",
    "\r\n",
    "- endpoint agents,\r\n",
    "- operating system logs,\r\n",
    "- EDR tools,\r\n",
    "- log forwarders.\r\n",
    "\r\n",
    "Here, the data is **synthetic**, but the **architecture is realistic**.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## Key Architectural Ideas Demonstrated\r\n",
    "\r\n",
    "### 1. Event-Driven Design\r\n",
    "\r\n",
    "Events are sent to Kafka **independently of consumers**.\r\n",
    "\r\n",
    "The producer:\r\n",
    "- does not know who will process the data,\r\n",
    "- does not wait for classification results,\r\n",
    "- does not depend on downstream components.\r\n",
    "\r\n",
    "This decoupling is a core principle of **high-load cybersecurity pipelines**.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 2. Message Queue Instead of Function Calls\r\n",
    "\r\n",
    "Kafka is used instead of direct function calls to ensure:\r\n",
    "\r\n",
    "- buffering under load,\r\n",
    "- fault tolerance,\r\n",
    "- asynchronous processing,\r\n",
    "- scalability.\r\n",
    "\r\n",
    "This is why real SOC pipelines rely on queues, not synchronous APIs.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 3. Distributed Tracing from the First Stage\r\n",
    "\r\n",
    "Each generated event is assigned:\r\n",
    "\r\n",
    "- a unique `event_id`,\r\n",
    "- a corresponding **trace ID**,\r\n",
    "- and a producer-side trace span.\r\n",
    "\r\n",
    "This allows the **entire lifecycle of one event** to be observed later in Jaeger,\r\n",
    "across multiple pipeline stages.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## Generated Event Structure\r\n",
    "\r\n",
    "Each event is a JSON object with fields such as:\r\n",
    "\r\n",
    "- `event_id`\r\n",
    "- `timestamp`\r\n",
    "- `user`\r\n",
    "- `host`\r\n",
    "- `source_ip`\r\n",
    "- `event_type`\r\n",
    "\r\n",
    "Depending on the event type, additional fields are included:\r\n",
    "\r\n",
    "- process execution details, or\r\n",
    "- authentication result details.\r\n",
    "\r\n",
    "This structure is intentionally simple and readable.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## Tracing Model Used Here\r\n",
    "\r\n",
    "This producer uses **OpenTelemetry** to emit traces.\r\n",
    "\r\n",
    "Important design choice:\r\n",
    "\r\n",
    "> **The trace ID is derived from the event ID.**\r\n",
    "\r\n",
    "This ensures that:\r\n",
    "- producer spans,\r\n",
    "- consumer spans,\r\n",
    "- and processing spans\r\n",
    "\r\n",
    "all appear as part of **one single trace** in Jaeger.\r\n",
    "\r\n",
    "This is essential for understanding **end-to-end pipeline execution**.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## What This Notebook Does NOT Do\r\n",
    "\r\n",
    "- It does **not** classify events.\r\n",
    "- It does **not** store results.\r\n",
    "- It does **not** perform analytics.\r\n",
    "- It does **not** optimize performance.\r\n",
    "\r\n",
    "All of that happens in downstream pipeline stages.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## How This Notebook Is Used in the Lab\r\n",
    "\r\n",
    "1. Start the full pipeline using `docker compose`.\r\n",
    "2. Run this notebook to begin producing events.\r\n",
    "3. Observe:\r\n",
    "   - events appearing in Kafka (via Redpanda Console),\r\n",
    "   - traces appearing in Jaeger.\r\n",
    "4. Run the consumer notebook in parallel.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## Key Takeaway\r\n",
    "\r\n",
    "> **This notebook models how security data enters a real pipeline.**  \r\n",
    "> Understanding this stage is critical before working on detection, ML, or performance.\r\n",
    "\r\n",
    "In cybersecurity systems, **data ingestion is as important as analysis**.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26040d73-33d5-44ed-a725-0976a6a5df2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q kafka-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fbac2d6-16a8-4be9-93eb-0dc965728ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1768469899.066104    4703 backup_poller.cc:138] Run client channel backup poller: UNKNOWN:pollset_work {children:[UNKNOWN:epoll_wait: Bad file descriptor (9)]}\n"
     ]
    }
   ],
   "source": [
    "!pip install -q opentelemetry-sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c4ee04f-0a42-4340-a829-ad9c73c54340",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q opentelemetry-exporter-otlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b055b6b-be81-4ec3-abbb-da971c9d58d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import time\n",
    "import uuid\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "from kafka import KafkaProducer\n",
    "\n",
    "# ---------------- OpenTelemetry ----------------\n",
    "from opentelemetry import trace\n",
    "from opentelemetry.sdk.resources import Resource\n",
    "from opentelemetry.sdk.trace import TracerProvider\n",
    "from opentelemetry.sdk.trace.export import BatchSpanProcessor\n",
    "from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import (\n",
    "    OTLPSpanExporter,\n",
    ")\n",
    "from opentelemetry.trace import SpanKind, TraceFlags\n",
    "from opentelemetry.trace import set_span_in_context\n",
    "from opentelemetry.trace import SpanContext, NonRecordingSpan\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Configuration\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "KAFKA_BOOTSTRAP_SERVERS = \"kafka:9092\"\n",
    "KAFKA_TOPIC = \"raw-events\"\n",
    "\n",
    "# OpenTelemetry Collector endpoint\n",
    "OTLP_ENDPOINT = \"jaeger:4317\"  # gRPC\n",
    "SERVICE_NAME = \"windows-log-producer\"\n",
    "\n",
    "EVENT_INTERVAL_SEC = (1, 5)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Tracing setup (OTLP)\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "resource = Resource.create(\n",
    "    {\n",
    "        \"service.name\": SERVICE_NAME,\n",
    "    }\n",
    ")\n",
    "\n",
    "trace.set_tracer_provider(TracerProvider(resource=resource))\n",
    "tracer = trace.get_tracer(__name__)\n",
    "\n",
    "otlp_exporter = OTLPSpanExporter(\n",
    "    endpoint=OTLP_ENDPOINT,\n",
    "    insecure=True,  # внутри docker-сети\n",
    ")\n",
    "\n",
    "span_processor = BatchSpanProcessor(otlp_exporter)\n",
    "trace.get_tracer_provider().add_span_processor(span_processor)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Kafka producer\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,\n",
    "    value_serializer=lambda v: json.dumps(v).encode(\"utf-8\"),\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Event generation\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "USERS = [\"alice\", \"bob\", \"charlie\", \"admin\"]\n",
    "HOSTS = [\"win-01\", \"win-02\", \"win-03\"]\n",
    "IPS = [\"10.0.0.10\", \"10.0.0.11\", \"10.0.0.12\"]\n",
    "\n",
    "PROCESS_SCENARIOS = [\n",
    "    {\n",
    "        \"process_name\": \"powershell.exe\",\n",
    "        \"command_line\": \"powershell -EncodedCommand SQBFAFgA\",\n",
    "        \"parent_process\": \"explorer.exe\",\n",
    "    },\n",
    "    {\n",
    "        \"process_name\": \"cmd.exe\",\n",
    "        \"command_line\": \"cmd.exe /c whoami\",\n",
    "        \"parent_process\": \"explorer.exe\",\n",
    "    },\n",
    "    {\n",
    "        \"process_name\": \"notepad.exe\",\n",
    "        \"command_line\": \"notepad.exe\",\n",
    "        \"parent_process\": \"explorer.exe\",\n",
    "    },\n",
    "]\n",
    "\n",
    "LOGIN_SCENARIOS = [\n",
    "    {\"logon_type\": \"success\"},\n",
    "    {\"logon_type\": \"failure\"},\n",
    "]\n",
    "\n",
    "def generate_event():\n",
    "    event_id = str(uuid.uuid4())\n",
    "    event_type = random.choice([\"process_start\", \"user_login\"])\n",
    "\n",
    "    event = {\n",
    "        \"event_id\": event_id,\n",
    "        \"timestamp\": datetime.now(timezone.utc).isoformat(),\n",
    "        \"user\": random.choice(USERS),\n",
    "        \"host\": random.choice(HOSTS),\n",
    "        \"source_ip\": random.choice(IPS),\n",
    "        \"event_type\": event_type,\n",
    "    }\n",
    "\n",
    "    if event_type == \"process_start\":\n",
    "        s = random.choice(PROCESS_SCENARIOS)\n",
    "        event.update(\n",
    "            {\n",
    "                \"process_name\": s[\"process_name\"],\n",
    "                \"command_line\": s[\"command_line\"],\n",
    "                \"parent_process\": s[\"parent_process\"],\n",
    "                \"logon_type\": None,\n",
    "            }\n",
    "        )\n",
    "    else:\n",
    "        s = random.choice(LOGIN_SCENARIOS)\n",
    "        event.update(\n",
    "            {\n",
    "                \"process_name\": None,\n",
    "                \"command_line\": None,\n",
    "                \"parent_process\": None,\n",
    "                \"logon_type\": s[\"logon_type\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return event\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Main loop\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "print(\"Starting Windows log producer (OTLP)...\")\n",
    "print(f\"Kafka topic: {KAFKA_TOPIC}\")\n",
    "print(f\"OTLP endpoint: {OTLP_ENDPOINT}\")\n",
    "\n",
    "while True:\n",
    "    event = generate_event()\n",
    "    event_id = event[\"event_id\"]\n",
    "\n",
    "    # event_id → trace_id (UUID → 128-bit int)\n",
    "    trace_id = uuid.UUID(event_id).int\n",
    "\n",
    "    parent_ctx = set_span_in_context(\n",
    "        NonRecordingSpan(\n",
    "            SpanContext(\n",
    "                trace_id=trace_id,\n",
    "                span_id=random.getrandbits(64),\n",
    "                is_remote=False,\n",
    "                trace_flags=TraceFlags(TraceFlags.SAMPLED),\n",
    "                trace_state={},\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    with tracer.start_as_current_span(\n",
    "        \"produce_event\",\n",
    "        context=parent_ctx,\n",
    "        kind=SpanKind.PRODUCER,\n",
    "    ) as span:\n",
    "\n",
    "        span.set_attribute(\"event.id\", event_id)\n",
    "        span.set_attribute(\"event.type\", event[\"event_type\"])\n",
    "        span.set_attribute(\"host.name\", event[\"host\"])\n",
    "        span.set_attribute(\"user.name\", event[\"user\"])\n",
    "\n",
    "        with tracer.start_as_current_span(\"kafka_produce\"):\n",
    "            producer.send(KAFKA_TOPIC, event)\n",
    "            producer.flush()\n",
    "\n",
    "        print(f\"Produced event {event_id} ({event['event_type']})\")\n",
    "\n",
    "    time.sleep(random.uniform(*EVENT_INTERVAL_SEC))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
