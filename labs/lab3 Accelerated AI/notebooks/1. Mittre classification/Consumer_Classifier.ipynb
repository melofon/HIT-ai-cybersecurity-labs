{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0f1d441-9168-4959-955f-ad0dc0ce5423",
   "metadata": {},
   "source": [
    "# Event Consumer & MITRE ATT&CK Classifier\r\n",
    "\r\n",
    "## Purpose of This Notebook\r\n",
    "\r\n",
    "This notebook implements the **consumer and classification stage** of the cybersecurity pipeline.\r\n",
    "\r\n",
    "Its responsibility is to:\r\n",
    "\r\n",
    "- consume security events from Kafka,\r\n",
    "- classify them according to the **MITRE ATT&CK framework**,\r\n",
    "- emit **distributed tracing information**,\r\n",
    "- and persist classification results for later analysis.\r\n",
    "\r\n",
    "This notebook represents the **core analytical stage** of the pipeline.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## Role in the Overall Pipeline\r\n",
    "\r\n",
    "Conceptually, this notebook corresponds to the **detection and enrichment layer** in a SOC pipeline.\r\n",
    "\r\n",
    "In real-world systems, this stage would include:\r\n",
    "\r\n",
    "- rule-based detection,\r\n",
    "- machine learning models,\r\n",
    "- correlation logic,\r\n",
    "- enrichment with external context.\r\n",
    "\r\n",
    "Here, the logic is intentionally simplified to keep the focus on **pipeline architecture**, not detection accuracy.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## Key Architectural Ideas Demonstrated\r\n",
    "\r\n",
    "### 1. Asynchronous Consumption from a Queue\r\n",
    "\r\n",
    "The consumer reads events from Kafka using a **consumer group**.\r\n",
    "\r\n",
    "Important implications:\r\n",
    "\r\n",
    "- the consumer is decoupled from the producer,\r\n",
    "- it can be restarted without data loss,\r\n",
    "- multiple consumers could be added to scale processing.\r\n",
    "\r\n",
    "This mirrors how real SOC pipelines handle high event volumes.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 2. Stateless Processing with Persistent Output\r\n",
    "\r\n",
    "The consumer:\r\n",
    "\r\n",
    "- does **not** store internal state,\r\n",
    "- processes each event independently,\r\n",
    "- writes results to a persistent output file.\r\n",
    "\r\n",
    "This design makes the pipeline:\r\n",
    "\r\n",
    "- easier to reason about,\r\n",
    "- easier to scale,\r\n",
    "- easier to debug.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 3. MITRE ATT&CK as a Classification Vocabulary\r\n",
    "\r\n",
    "Each event is mapped to:\r\n",
    "\r\n",
    "- a **MITRE ATT&CK tactic** (high-level goal),\r\n",
    "- a **MITRE ATT&CK technique** (concrete method).\r\n",
    "\r\n",
    "The classification logic is **rule-based and minimal by design**.\r\n",
    "\r\n",
    "The goal is not accurate detection, but understanding:\r\n",
    "\r\n",
    "> **how cybersecurity events are normalized and labeled in pipelines**.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## Tracing Model Used Here\r\n",
    "\r\n",
    "This notebook continues the **end-to-end trace** started by the producer.\r\n",
    "\r\n",
    "Key design choices:\r\n",
    "\r\n",
    "- the `trace_id` is reconstructed from `event_id`,\r\n",
    "- consumer spans become **children of producer spans**,\r\n",
    "- each processing step is represented as a separate span.\r\n",
    "\r\n",
    "As a result, one event appears in Jaeger as **one complete pipeline trace**.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## What Is Traced\r\n",
    "\r\n",
    "For each consumed event, the following spans are emitted:\r\n",
    "\r\n",
    "- `consume_event` — message consumption from Kafka\r\n",
    "- `classify_event` — MITRE ATT&CK classification logic\r\n",
    "- `write_csv` — persistence of classification results\r\n",
    "\r\n",
    "Each span contains stceleration, or performance optimization.\r\n",
    "TRE tactic and technique.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## Output Format\r\n",
    "\r\n",
    "Classification results are written to a local CSV file:\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62702db-10d4-4bf2-bb96-18f47532e3b2",
   "metadata": {},
   "source": [
    "## Output Format\n",
    "\n",
    "Classification results are written to a local CSV file: `classified_packets.csv`\n",
    "\n",
    "This file acts as a **materialized view** of the pipeline output.\n",
    "\n",
    "CSV is used intentionally because:\n",
    "\n",
    "- it requires no database setup,\n",
    "- it is easy to inspect,\n",
    "- it integrates naturally with Jupyter for analysis.\n",
    "\n",
    "---\n",
    "\n",
    "## What This Notebook Does NOT Do\n",
    "\n",
    "- It does **not** train machine learning models.\n",
    "- It does **not** optimize throughput or latency.\n",
    "- It does **not** perform visualization.\n",
    "- It does **not** correlate events across time.\n",
    "\n",
    "These aspects are addressed in later notebooks and labs.\n",
    "\n",
    "---\n",
    "\n",
    "## How This Notebook Is Used in the Lab\n",
    "\n",
    "1. Start the full pipeline using `docker compose`.\n",
    "2. Run the producer notebook to generate events.\n",
    "3. Run this notebook to consume and classify events.\n",
    "4. Observe:\n",
    "   - classified results accumulating in CSV,\n",
    "   - end-to-end traces appearing in Jaeger.\n",
    "\n",
    "This notebook should run **in parallel** with the producer.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Takeaway\n",
    "\n",
    "> **This notebook shows how raw security telemetry becomes structured detection output.**\n",
    "\n",
    "In real cybersecurity systems, this transformation step is critical:\n",
    "it connects low-level events to high-level security meaning.\n",
    "\n",
    "Understanding this stage is essential before introducing\n",
    "machine learning, GPU acceleration, or performance optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd275f8e-a83a-4742-9055-1b357df18b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import os\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "from kafka import KafkaConsumer\n",
    "\n",
    "# ---------------- OpenTelemetry ----------------\n",
    "from opentelemetry import trace\n",
    "from opentelemetry.sdk.resources import Resource\n",
    "from opentelemetry.sdk.trace import TracerProvider\n",
    "from opentelemetry.sdk.trace.export import BatchSpanProcessor\n",
    "from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import (\n",
    "    OTLPSpanExporter,\n",
    ")\n",
    "from opentelemetry.trace import SpanKind, TraceFlags\n",
    "from opentelemetry.trace import set_span_in_context\n",
    "from opentelemetry.trace import SpanContext, NonRecordingSpan\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Configuration\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "KAFKA_BOOTSTRAP_SERVERS = \"kafka:9092\"\n",
    "KAFKA_TOPIC = \"raw-events\"\n",
    "KAFKA_GROUP_ID = \"packet-classifier\"\n",
    "\n",
    "OTLP_ENDPOINT = \"jaeger:4317\"\n",
    "SERVICE_NAME = \"packet-classifier\"\n",
    "\n",
    "OUTPUT_CSV = \"classified_packets.csv\"\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Tracing setup (OTLP → Jaeger)\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "resource = Resource.create(\n",
    "    {\n",
    "        \"service.name\": SERVICE_NAME,\n",
    "    }\n",
    ")\n",
    "\n",
    "trace.set_tracer_provider(TracerProvider(resource=resource))\n",
    "tracer = trace.get_tracer(__name__)\n",
    "\n",
    "otlp_exporter = OTLPSpanExporter(\n",
    "    endpoint=OTLP_ENDPOINT,\n",
    "    insecure=True,\n",
    ")\n",
    "\n",
    "span_processor = BatchSpanProcessor(otlp_exporter)\n",
    "trace.get_tracer_provider().add_span_processor(span_processor)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Kafka consumer\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "consumer = KafkaConsumer(\n",
    "    KAFKA_TOPIC,\n",
    "    bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,\n",
    "    group_id=KAFKA_GROUP_ID,\n",
    "    auto_offset_reset=\"earliest\",\n",
    "    enable_auto_commit=True,\n",
    "    value_deserializer=lambda v: json.loads(v.decode(\"utf-8\")),\n",
    ")\n",
    "\n",
    "print(\"Kafka consumer started\")\n",
    "print(f\"Topic: {KAFKA_TOPIC}\")\n",
    "print(f\"OTLP endpoint: {OTLP_ENDPOINT}\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# MITRE ATT&CK classification (toy logic)\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def classify_event(event):\n",
    "    \"\"\"\n",
    "    Very simple rule-based MITRE ATT&CK classification.\n",
    "    This is INTENTIONALLY simplistic — focus is on the pipeline.\n",
    "    \"\"\"\n",
    "\n",
    "    if event[\"event_type\"] == \"process_start\":\n",
    "        cmd = (event.get(\"command_line\") or \"\").lower()\n",
    "\n",
    "        if \"encodedcommand\" in cmd:\n",
    "            return \"TA0002\", \"T1059.001\"  # Execution / PowerShell\n",
    "        if \"cmd.exe\" in cmd:\n",
    "            return \"TA0002\", \"T1059.003\"  # Execution / Windows Command Shell\n",
    "        return \"TA0002\", \"T1059\"\n",
    "\n",
    "    if event[\"event_type\"] == \"user_login\":\n",
    "        if event.get(\"logon_type\") == \"failure\":\n",
    "            return \"TA0006\", \"T1110\"  # Credential Access / Brute Force\n",
    "        return \"TA0001\", \"T1078\"      # Initial Access / Valid Accounts\n",
    "\n",
    "    return \"TA0000\", \"T0000\"\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# CSV initialization\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "file_exists = os.path.exists(OUTPUT_CSV)\n",
    "\n",
    "csv_file = open(OUTPUT_CSV, \"a\", newline=\"\")\n",
    "writer = csv.DictWriter(\n",
    "    csv_file,\n",
    "    fieldnames=[\n",
    "        \"event_id\",\n",
    "        \"timestamp\",\n",
    "        \"user\",\n",
    "        \"host\",\n",
    "        \"source_ip\",\n",
    "        \"event_type\",\n",
    "        \"mitre_tactic\",\n",
    "        \"mitre_technique\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "if not file_exists:\n",
    "    writer.writeheader()\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Main consume loop\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "for msg in consumer:\n",
    "    event = msg.value\n",
    "    event_id = event[\"event_id\"]\n",
    "\n",
    "    # Restore trace context from event_id\n",
    "    trace_id = int(event_id.replace(\"-\", \"\"), 16)\n",
    "\n",
    "    parent_ctx = set_span_in_context(\n",
    "        NonRecordingSpan(\n",
    "            SpanContext(\n",
    "                trace_id=trace_id,\n",
    "                span_id=random.getrandbits(64),\n",
    "                is_remote=True,\n",
    "                trace_flags=TraceFlags(TraceFlags.SAMPLED),\n",
    "                trace_state={},\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    with tracer.start_as_current_span(\n",
    "        \"consume_event\",\n",
    "        context=parent_ctx,\n",
    "        kind=SpanKind.CONSUMER,\n",
    "    ) as span:\n",
    "\n",
    "        span.set_attribute(\"event.id\", event_id)\n",
    "        span.set_attribute(\"event.type\", event[\"event_type\"])\n",
    "        span.set_attribute(\"host.name\", event[\"host\"])\n",
    "        span.set_attribute(\"user.name\", event[\"user\"])\n",
    "\n",
    "        with tracer.start_as_current_span(\"classify_event\"):\n",
    "            tactic, technique = classify_event(event)\n",
    "\n",
    "        span.set_attribute(\"mitre.tactic\", tactic)\n",
    "        span.set_attribute(\"mitre.technique\", technique)\n",
    "\n",
    "        record = {\n",
    "            \"event_id\": event_id,\n",
    "            \"timestamp\": event[\"timestamp\"],\n",
    "            \"user\": event[\"user\"],\n",
    "            \"host\": event[\"host\"],\n",
    "            \"source_ip\": event[\"source_ip\"],\n",
    "            \"event_type\": event[\"event_type\"],\n",
    "            \"mitre_tactic\": tactic,\n",
    "            \"mitre_technique\": technique,\n",
    "        }\n",
    "\n",
    "        with tracer.start_as_current_span(\"write_csv\"):\n",
    "            writer.writerow(record)\n",
    "            csv_file.flush()\n",
    "\n",
    "        print(\n",
    "            f\"Processed {event_id} → {tactic} / {technique}\"\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
